{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DS-NERF.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Er8zySn7r1J0yXNZd6zZUCq1eOAymvCI",
      "authorship_tag": "ABX9TyNDnmskrYubksfReAM6XJWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericstheguy/Colab-Notebooks/blob/main/DS_NERF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v94NTaXP1qwD"
      },
      "source": [
        "# ***DS-NeRF in Colab***\n",
        "* Demo Train / Render.\n",
        "* https://github.com/dunbar12138/DSNeRF\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRLyBOBhH8Og"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkHTdIBsxAh"
      },
      "source": [
        "## **Prepare Custom Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdMSDuDr2oMe",
        "cellView": "form"
      },
      "source": [
        "#@title 1. Install Dependencies (for 2 & 3)\n",
        "!sudo apt-get install \\\n",
        "    git \\\n",
        "    cmake \\\n",
        "    build-essential \\\n",
        "    libboost-program-options-dev \\\n",
        "    libboost-filesystem-dev \\\n",
        "    libboost-graph-dev \\\n",
        "    libboost-regex-dev \\\n",
        "    libboost-system-dev \\\n",
        "    libboost-test-dev \\\n",
        "    libeigen3-dev \\\n",
        "    libsuitesparse-dev \\\n",
        "    libfreeimage-dev \\\n",
        "    libgoogle-glog-dev \\\n",
        "    libgflags-dev \\\n",
        "    libglew-dev \\\n",
        "    qtbase5-dev \\\n",
        "    libqt5opengl5-dev \\\n",
        "    libcgal-dev \\\n",
        "    libcgal-qt5-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KtpTgZSHs5FA"
      },
      "source": [
        "#@title 2. Install Ceres-solver\n",
        "!sudo apt-get install libatlas-base-dev libsuitesparse-dev\n",
        "!git clone https://ceres-solver.googlesource.com/ceres-solver\n",
        "%cd ceres-solver\n",
        "!git checkout $(git describe --tags) # Checkout the latest release\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake .. -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF\n",
        "!make\n",
        "!sudo make install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "sQSehAv2tEkS"
      },
      "source": [
        "#@title 3. Install Colmap \n",
        "!git clone https://github.com/colmap/colmap\n",
        "%cd colmap\n",
        "!git checkout dev\n",
        "%mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make\n",
        "!sudo make install\n",
        "!CC=/usr/bin/gcc-6 CXX=/usr/bin/g++-6 cmake .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI5-p1o2rvkv"
      },
      "source": [
        "**Position files in drive like this:**\n",
        "<p>├ My-Drive\n",
        "<p>├── data\n",
        "<p>│   ├── custom\n",
        "<p>│   ├── ├── images\n",
        "<p>│   ├── ├── ├── _.png/jpg\n",
        "<p>│   ├── ├── ├── _.png/jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8YBWelOqQxU",
        "cellView": "form"
      },
      "source": [
        "#@title Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPHotnHJwOCC",
        "cellView": "form"
      },
      "source": [
        "#@title Run Colmap\n",
        "%cd /content\n",
        "!git clone https://github.com/Fyusion/LLFF\n",
        "%cd /content/LLFF\n",
        "!python /content/LLFF/imgs2poses.py \"/content/drive/MyDrive/data/custom\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJVrXT7urVUG"
      },
      "source": [
        "*Colmap producing an error likely means the images were not able to process.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGfZ6Mu3HkQn"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayy0kffHP5K"
      },
      "source": [
        "## **Prepare DS-NeRF**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzYDr4rCHhOW",
        "cellView": "form"
      },
      "source": [
        "#@title Install | DS-NeRF\n",
        "%cd /content\n",
        "!git clone https://github.com/dunbar12138/DSNeRF\n",
        "%cd DSNeRF\n",
        "!pip install -r requirements.txt\n",
        "!mkdir /content/drive/MyDrive/data/custom-log\n",
        "open(\"/content/config.txt\",\"w+\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHnFPbKKgOt",
        "cellView": "form"
      },
      "source": [
        "#@title Install | Imagemagick\n",
        "!sudo apt install php php-common gcc\n",
        "!sudo apt install imagemagick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNnNa3TRHhOW",
        "cellView": "form"
      },
      "source": [
        "#@title Remove | Torchasserted Import\n",
        "%%writefile /content/DSNeRF/run_nerf_helpers.py\n",
        "import torch\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Misc\n",
        "img2mse = lambda x, y : torch.mean((x - y) ** 2)\n",
        "mse2psnr = lambda x : -10. * torch.log(x) / torch.log(torch.Tensor([10.]))\n",
        "to8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n",
        "\n",
        "\n",
        "# Positional encoding (section 5.1)\n",
        "class Embedder:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.kwargs = kwargs\n",
        "        self.create_embedding_fn()\n",
        "        \n",
        "    def create_embedding_fn(self):\n",
        "        embed_fns = []\n",
        "        d = self.kwargs['input_dims']\n",
        "        out_dim = 0\n",
        "        if self.kwargs['include_input']:\n",
        "            embed_fns.append(lambda x : x)\n",
        "            out_dim += d\n",
        "            \n",
        "        max_freq = self.kwargs['max_freq_log2']\n",
        "        N_freqs = self.kwargs['num_freqs']\n",
        "        \n",
        "        if self.kwargs['log_sampling']:\n",
        "            freq_bands = 2.**torch.linspace(0., max_freq, steps=N_freqs)\n",
        "        else:\n",
        "            freq_bands = torch.linspace(2.**0., 2.**max_freq, steps=N_freqs)\n",
        "            \n",
        "        for freq in freq_bands:\n",
        "            for p_fn in self.kwargs['periodic_fns']:\n",
        "                embed_fns.append(lambda x, p_fn=p_fn, freq=freq : p_fn(x * freq))\n",
        "                out_dim += d\n",
        "                    \n",
        "        self.embed_fns = embed_fns\n",
        "        self.out_dim = out_dim\n",
        "        \n",
        "    def embed(self, inputs):\n",
        "        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)\n",
        "\n",
        "\n",
        "def get_embedder(multires, i=0):\n",
        "    if i == -1:\n",
        "        return nn.Identity(), 3\n",
        "    \n",
        "    embed_kwargs = {\n",
        "                'include_input' : True,\n",
        "                'input_dims' : 3,\n",
        "                'max_freq_log2' : multires-1,\n",
        "                'num_freqs' : multires,\n",
        "                'log_sampling' : True,\n",
        "                'periodic_fns' : [torch.sin, torch.cos],\n",
        "    }\n",
        "    \n",
        "    embedder_obj = Embedder(**embed_kwargs)\n",
        "    embed = lambda x, eo=embedder_obj : eo.embed(x)\n",
        "    return embed, embedder_obj.out_dim\n",
        "\n",
        "\n",
        "# Model\n",
        "class NeRF(nn.Module):\n",
        "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False):\n",
        "        \"\"\" \n",
        "        \"\"\"\n",
        "        super(NeRF, self).__init__()\n",
        "        self.D = D\n",
        "        self.W = W\n",
        "        self.input_ch = input_ch\n",
        "        self.input_ch_views = input_ch_views\n",
        "        self.skips = skips\n",
        "        self.use_viewdirs = use_viewdirs\n",
        "        \n",
        "        self.pts_linears = nn.ModuleList(\n",
        "            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n",
        "        \n",
        "        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n",
        "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n",
        "\n",
        "        ### Implementation according to the paper\n",
        "        # self.views_linears = nn.ModuleList(\n",
        "        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n",
        "        \n",
        "        if use_viewdirs:\n",
        "            self.feature_linear = nn.Linear(W, W)\n",
        "            self.alpha_linear = nn.Linear(W, 1)\n",
        "            self.rgb_linear = nn.Linear(W//2, 3)\n",
        "        else:\n",
        "            self.output_linear = nn.Linear(W, output_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n",
        "        h = input_pts\n",
        "        for i, l in enumerate(self.pts_linears):\n",
        "            h = self.pts_linears[i](h)\n",
        "            h = F.relu(h)\n",
        "            if i in self.skips:\n",
        "                h = torch.cat([input_pts, h], -1)\n",
        "\n",
        "        if self.use_viewdirs:\n",
        "            alpha = self.alpha_linear(h)\n",
        "            feature = self.feature_linear(h)\n",
        "            h = torch.cat([feature, input_views], -1)\n",
        "        \n",
        "            for i, l in enumerate(self.views_linears):\n",
        "                h = self.views_linears[i](h)\n",
        "                h = F.relu(h)\n",
        "\n",
        "            rgb = self.rgb_linear(h)\n",
        "            outputs = torch.cat([rgb, alpha], -1)\n",
        "        else:\n",
        "            outputs = self.output_linear(h)\n",
        "\n",
        "        return outputs    \n",
        "\n",
        "    def load_weights_from_keras(self, weights):\n",
        "        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n",
        "        \n",
        "        # Load pts_linears\n",
        "        for i in range(self.D):\n",
        "            idx_pts_linears = 2 * i\n",
        "            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))    \n",
        "            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n",
        "        \n",
        "        # Load feature_linear\n",
        "        idx_feature_linear = 2 * self.D\n",
        "        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n",
        "        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n",
        "\n",
        "        # Load views_linears\n",
        "        idx_views_linears = 2 * self.D + 2\n",
        "        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n",
        "        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n",
        "\n",
        "        # Load rgb_linear\n",
        "        idx_rbg_linear = 2 * self.D + 4\n",
        "        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n",
        "        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n",
        "\n",
        "        # Load alpha_linear\n",
        "        idx_alpha_linear = 2 * self.D + 6\n",
        "        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n",
        "        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))\n",
        "\n",
        "class NeRF_RGB(nn.Module):\n",
        "    def __init__(self, D=8, W=256, input_ch=3, input_ch_views=3, output_ch=4, skips=[4], use_viewdirs=False, alpha_model=None):\n",
        "        \"\"\" \n",
        "        \"\"\"\n",
        "        super(NeRF_RGB, self).__init__()\n",
        "        self.D = D\n",
        "        self.W = W\n",
        "        self.input_ch = input_ch\n",
        "        self.input_ch_views = input_ch_views\n",
        "        self.skips = skips\n",
        "        self.use_viewdirs = use_viewdirs\n",
        "        \n",
        "        self.pts_linears = nn.ModuleList(\n",
        "            [nn.Linear(input_ch, W)] + [nn.Linear(W, W) if i not in self.skips else nn.Linear(W + input_ch, W) for i in range(D-1)])\n",
        "        \n",
        "        ### Implementation according to the official code release (https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py#L104-L105)\n",
        "        self.views_linears = nn.ModuleList([nn.Linear(input_ch_views + W, W//2)])\n",
        "\n",
        "        ### Implementation according to the paper\n",
        "        # self.views_linears = nn.ModuleList(\n",
        "        #     [nn.Linear(input_ch_views + W, W//2)] + [nn.Linear(W//2, W//2) for i in range(D//2)])\n",
        "        \n",
        "        if use_viewdirs:\n",
        "            self.feature_linear = nn.Linear(W, W)\n",
        "            # self.alpha_linear = nn.Linear(W, 1)\n",
        "            self.rgb_linear = nn.Linear(W//2, 3)\n",
        "        else:\n",
        "            self.output_linear = nn.Linear(W, output_ch)\n",
        "\n",
        "        self.alpha_model = alpha_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_pts, input_views = torch.split(x, [self.input_ch, self.input_ch_views], dim=-1)\n",
        "        h = input_pts\n",
        "        for i, l in enumerate(self.pts_linears):\n",
        "            h = self.pts_linears[i](h)\n",
        "            h = F.relu(h)\n",
        "            if i in self.skips:\n",
        "                h = torch.cat([input_pts, h], -1)\n",
        "\n",
        "        if self.use_viewdirs:\n",
        "            with torch.no_grad():\n",
        "                alpha = self.alpha_model(x)[...,3][...,None]\n",
        "            feature = self.feature_linear(h)\n",
        "            h = torch.cat([feature, input_views], -1)\n",
        "        \n",
        "            for i, l in enumerate(self.views_linears):\n",
        "                h = self.views_linears[i](h)\n",
        "                h = F.relu(h)\n",
        "\n",
        "            rgb = self.rgb_linear(h)\n",
        "            outputs = torch.cat([rgb, alpha], -1)\n",
        "        else:\n",
        "            outputs = self.output_linear(h)\n",
        "\n",
        "        return outputs    \n",
        "\n",
        "    def load_weights_from_keras(self, weights):\n",
        "        assert self.use_viewdirs, \"Not implemented if use_viewdirs=False\"\n",
        "        \n",
        "        # Load pts_linears\n",
        "        for i in range(self.D):\n",
        "            idx_pts_linears = 2 * i\n",
        "            self.pts_linears[i].weight.data = torch.from_numpy(np.transpose(weights[idx_pts_linears]))    \n",
        "            self.pts_linears[i].bias.data = torch.from_numpy(np.transpose(weights[idx_pts_linears+1]))\n",
        "        \n",
        "        # Load feature_linear\n",
        "        idx_feature_linear = 2 * self.D\n",
        "        self.feature_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_feature_linear]))\n",
        "        self.feature_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_feature_linear+1]))\n",
        "\n",
        "        # Load views_linears\n",
        "        idx_views_linears = 2 * self.D + 2\n",
        "        self.views_linears[0].weight.data = torch.from_numpy(np.transpose(weights[idx_views_linears]))\n",
        "        self.views_linears[0].bias.data = torch.from_numpy(np.transpose(weights[idx_views_linears+1]))\n",
        "\n",
        "        # Load rgb_linear\n",
        "        idx_rbg_linear = 2 * self.D + 4\n",
        "        self.rgb_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear]))\n",
        "        self.rgb_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_rbg_linear+1]))\n",
        "\n",
        "        # Load alpha_linear\n",
        "        idx_alpha_linear = 2 * self.D + 6\n",
        "        self.alpha_linear.weight.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear]))\n",
        "        self.alpha_linear.bias.data = torch.from_numpy(np.transpose(weights[idx_alpha_linear+1]))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Ray helpers\n",
        "def get_rays(H, W, focal, c2w):\n",
        "    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H))  # pytorch's meshgrid has indexing='ij'\n",
        "    i = i.t()\n",
        "    j = j.t()\n",
        "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
        "    # Rotate ray directions from camera frame to the world frame\n",
        "    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
        "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
        "    rays_o = c2w[:3,-1].expand(rays_d.shape)\n",
        "    return rays_o, rays_d\n",
        "\n",
        "\n",
        "def get_rays_np(H, W, focal, c2w):\n",
        "    i, j = np.meshgrid(np.arange(W, dtype=np.float32), np.arange(H, dtype=np.float32), indexing='xy') # i: H x W, j: H x W\n",
        "    dirs = np.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -np.ones_like(i)], -1) # dirs: H x W x 3\n",
        "    # Rotate ray directions from camera frame to the world frame\n",
        "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]\n",
        "    # Translate camera frame's origin to the world frame. It is the origin of all rays.\n",
        "    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))  # H x W x 3\n",
        "    return rays_o, rays_d\n",
        "\n",
        "\n",
        "def get_rays_by_coord_np(H, W, focal, c2w, coords):\n",
        "    i, j = (coords[:,0]-W*0.5)/focal, -(coords[:,1]-H*0.5)/focal\n",
        "    dirs = np.stack([i,j,-np.ones_like(i)],-1)\n",
        "    rays_d = np.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)\n",
        "    rays_o = np.broadcast_to(c2w[:3,-1], np.shape(rays_d))\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def ndc_rays(H, W, focal, near, rays_o, rays_d):\n",
        "    # Shift ray origins to near plane\n",
        "    t = -(near + rays_o[...,2]) / rays_d[...,2]\n",
        "    rays_o = rays_o + t[...,None] * rays_d\n",
        "    \n",
        "    # Projection\n",
        "    o0 = -1./(W/(2.*focal)) * rays_o[...,0] / rays_o[...,2]\n",
        "    o1 = -1./(H/(2.*focal)) * rays_o[...,1] / rays_o[...,2]\n",
        "    o2 = 1. + 2. * near / rays_o[...,2]\n",
        "\n",
        "    d0 = -1./(W/(2.*focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])\n",
        "    d1 = -1./(H/(2.*focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])\n",
        "    d2 = -2. * near / rays_o[...,2]\n",
        "    \n",
        "    rays_o = torch.stack([o0,o1,o2], -1)\n",
        "    rays_d = torch.stack([d0,d1,d2], -1)\n",
        "    \n",
        "    return rays_o, rays_d\n",
        "\n",
        "\n",
        "# Hierarchical sampling (section 5.2)\n",
        "def sample_pdf(bins, weights, N_samples, det=False, pytest=False):\n",
        "    # Get pdf\n",
        "    weights = weights + 1e-5 # prevent nans\n",
        "    pdf = weights / torch.sum(weights, -1, keepdim=True)\n",
        "    cdf = torch.cumsum(pdf, -1)\n",
        "    cdf = torch.cat([torch.zeros_like(cdf[...,:1]), cdf], -1)  # (batch, len(bins))\n",
        "\n",
        "    # Take uniform samples\n",
        "    if det:\n",
        "        u = torch.linspace(0., 1., steps=N_samples)\n",
        "        u = u.expand(list(cdf.shape[:-1]) + [N_samples])\n",
        "    else:\n",
        "        u = torch.rand(list(cdf.shape[:-1]) + [N_samples])\n",
        "\n",
        "    # Pytest, overwrite u with numpy's fixed random numbers\n",
        "    if pytest:\n",
        "        np.random.seed(0)\n",
        "        new_shape = list(cdf.shape[:-1]) + [N_samples]\n",
        "        if det:\n",
        "            u = np.linspace(0., 1., N_samples)\n",
        "            u = np.broadcast_to(u, new_shape)\n",
        "        else:\n",
        "            u = np.random.rand(*new_shape)\n",
        "        u = torch.Tensor(u)\n",
        "\n",
        "    # Invert CDF\n",
        "    u = u.contiguous()\n",
        "    inds = searchsorted(cdf, u, side='right')\n",
        "    below = torch.max(torch.zeros_like(inds-1), inds-1)\n",
        "    above = torch.min((cdf.shape[-1]-1) * torch.ones_like(inds), inds)\n",
        "    inds_g = torch.stack([below, above], -1)  # (batch, N_samples, 2)\n",
        "\n",
        "    # cdf_g = tf.gather(cdf, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n",
        "    # bins_g = tf.gather(bins, inds_g, axis=-1, batch_dims=len(inds_g.shape)-2)\n",
        "    matched_shape = [inds_g.shape[0], inds_g.shape[1], cdf.shape[-1]]\n",
        "    cdf_g = torch.gather(cdf.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
        "    bins_g = torch.gather(bins.unsqueeze(1).expand(matched_shape), 2, inds_g)\n",
        "\n",
        "    denom = (cdf_g[...,1]-cdf_g[...,0])\n",
        "    denom = torch.where(denom<1e-5, torch.ones_like(denom), denom)\n",
        "    t = (u-cdf_g[...,0])/denom\n",
        "    samples = bins_g[...,0] + t * (bins_g[...,1]-bins_g[...,0])\n",
        "\n",
        "    return samples\n",
        "\n",
        "def raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False, pytest=False):\n",
        "    \"\"\"Transforms model's predictions to semantically meaningful values.\n",
        "    Args:\n",
        "        raw: [num_rays, num_samples along ray, 4]. Prediction from model.\n",
        "        z_vals: [num_rays, num_samples along ray]. Integration time.\n",
        "        rays_d: [num_rays, 3]. Direction of each ray.\n",
        "    Returns:\n",
        "        rgb_map: [num_rays, 3]. Estimated RGB color of a ray.\n",
        "        disp_map: [num_rays]. Disparity map. Inverse of depth map.\n",
        "        acc_map: [num_rays]. Sum of weights along each ray.\n",
        "        weights: [num_rays, num_samples]. Weights assigned to each sampled color.\n",
        "        depth_map: [num_rays]. Estimated distance to object.\n",
        "    \"\"\"\n",
        "    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n",
        "\n",
        "    dists = z_vals[...,1:] - z_vals[...,:-1]\n",
        "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)  # [N_rays, N_samples]\n",
        "\n",
        "    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n",
        "\n",
        "    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n",
        "    noise = 0.\n",
        "    if raw_noise_std > 0.:\n",
        "        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n",
        "\n",
        "        # Overwrite randomly sampled data if pytest\n",
        "        if pytest:\n",
        "            np.random.seed(0)\n",
        "            noise = np.random.rand(*list(raw[...,3].shape)) * raw_noise_std\n",
        "            noise = torch.Tensor(noise)\n",
        "\n",
        "    alpha = raw2alpha(raw[...,3] + noise, dists)  # [N_rays, N_samples]\n",
        "    # weights = alpha * tf.math.cumprod(1.-alpha + 1e-10, -1, exclusive=True)\n",
        "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
        "    rgb_map = torch.sum(weights[...,None] * rgb, -2)  # [N_rays, 3]\n",
        "\n",
        "    depth_map = torch.sum(weights * z_vals, -1)\n",
        "    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n",
        "    acc_map = torch.sum(weights, -1)\n",
        "\n",
        "    if white_bkgd:\n",
        "        rgb_map = rgb_map + (1.-acc_map[...,None])\n",
        "\n",
        "    return rgb_map, disp_map, acc_map, weights, depth_map\n",
        "\n",
        "\n",
        "def sample_sigma(rays_o, rays_d, viewdirs, network, z_vals, network_query):\n",
        "    # N_rays = rays_o.shape[0]\n",
        "    # N_samples = len(z_vals)\n",
        "    # z_vals = z_vals.expand([N_rays, N_samples])\n",
        "\n",
        "    pts = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None] # [N_rays, N_samples, 3]\n",
        "    raw = network_query(pts, viewdirs, network)\n",
        "\n",
        "    rgb = torch.sigmoid(raw[...,:3])  # [N_rays, N_samples, 3]\n",
        "    sigma = F.relu(raw[...,3])\n",
        "\n",
        "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d)\n",
        "\n",
        "    return rgb, sigma, depth_map\n",
        "\n",
        "\n",
        "def visualize_sigma(sigma, z_vals, filename):\n",
        "    plt.plot(z_vals, sigma)\n",
        "    plt.xlabel('z_vals')\n",
        "    plt.ylabel('sigma')\n",
        "    plt.savefig(filename)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxvDbmnRBiFj"
      },
      "source": [
        "## **Run App**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "dkcRICgQGW1d"
      },
      "source": [
        "#@title NeRF Help\n",
        "!python run_nerf.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofmrF5j_FvQ1"
      },
      "source": [
        "#@title Edit Config (Optional)\n",
        "%%writefile /content/config.txt\n",
        "expname = custom\n",
        "basedir = /content/drive/MyDrive/data/custom-log\n",
        "datadir = /content/drive/MyDrive/data/custom\n",
        "dataset_type = llff\n",
        "#Resolution \n",
        "factor = 4\n",
        "#spherify = True (For 360 Scenes)\n",
        "N_rand = 1024\n",
        "N_samples = 64\n",
        "N_importance = 64\n",
        "#Render Resolution (8 for fast preview.)\n",
        "render_factor = 8\n",
        "use_viewdirs = True\n",
        "raw_noise_std = 1e0\n",
        "\n",
        "chunk = 1024\n",
        "netchunk = 2048\n",
        "netdepth = 4\n",
        "\n",
        "i_testset = 1000\n",
        "i_video = 10000\n",
        "N_iters = 20000\n",
        "\n",
        "train_scene = [0, 1]\n",
        "test_scene = [-1]\n",
        "no_ndc = True\n",
        "colmap_depth = True\n",
        "depth_loss = True\n",
        "depth_lambda = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6RCXsr5GwxV"
      },
      "source": [
        "**Config arguments are shown in NeRF Help.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_lZjO_ARkbg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GamcgSQfJEq4"
      },
      "source": [
        "#@title Show Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61QiBihIhrVt",
        "cellView": "form"
      },
      "source": [
        "#@title Train DS-NeRF\n",
        "!python run_nerf.py --config /content/config.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hNqgIKO9RVPr"
      },
      "source": [
        "#@title Render DS-NeRF\n",
        "!python run_nerf.py --config /content/config.txt --render_only"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO5H1FF1QuZn"
      },
      "source": [
        "## ***Needs Work:***\n",
        "* *Out Of Memory Error when beginning training.*\n",
        "* *Help with ideal config for colab.*"
      ]
    }
  ]
}